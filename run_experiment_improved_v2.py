#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ”¹å–„ç‰ˆv2: ç‰¹å¾´é‡æ­£è¦åŒ–ã¨ãƒãƒ©ãƒ³ã‚¹èª¿æ•´
"""

import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MultiLabelBinarizer, MinMaxScaler, normalize
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from janome.tokenizer import Tokenizer
import warnings
warnings.filterwarnings('ignore')

plt.rcParams['font.family'] = 'DejaVu Sans'

print("="*80)
print("æ”¹å–„ç‰ˆv2: ãŠå°å ´ã‚°ãƒ«ãƒ¡æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ  - ç‰¹å¾´é‡æ­£è¦åŒ–ã¨ãƒãƒ©ãƒ³ã‚¹èª¿æ•´")
print("="*80)

# åˆæœŸåŒ–
print("\n[1/12] åˆæœŸåŒ–...")
tokenizer = Tokenizer()
print("âœ… å®Œäº†")

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
print("\n[2/12] ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿...")
CSV_FILE_PATH = '/home/user/B2-store-recommend/suku/odaiba_reviews_4.csv'
df = pd.read_csv(CSV_FILE_PATH)
print(f"âœ… {len(df):,}è¡Œ")

# ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
print("\n[3/12] ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°...")
df['star_rating'] = pd.to_numeric(df['star_rating'], errors='coerce')
df.dropna(subset=['star_rating', 'category', 'review_text'], inplace=True)
df['category_list'] = df['category'].apply(lambda x: [c.strip() for c in x.split(',') if c.strip()])
print(f"âœ… {len(df):,}è¡Œ")

# åº—èˆ—é›†ç´„
print("\n[4/12] åº—èˆ—é›†ç´„...")
shop_grouped = df.groupby('shop_name').agg({
    'shop_url': 'first',
    'category': 'first',
    'category_list': 'first',
    'star_rating': 'mean',
    'review_text': lambda x: ' '.join(x.dropna().astype(str))
}).reset_index()
print(f"âœ… {len(shop_grouped):,}åº—èˆ—")

# ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰
print("\n[5/12] ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰å®šç¾©...")
STOP_WORDS = set([
    'ã“ã¨', 'ã‚‚ã®', 'ã‚ˆã†', 'ãŸã‚', 'ã®', 'ã—', 'ã‚“', 'ã•ã‚“', 'ã“ã‚Œ', 'ãã‚Œ', 'ã‚ã‚Œ',
    'ã“ã®', 'ãã®', 'ã‚ã®', 'ã“ã“', 'ãã“', 'ã‚ãã“', 'ä»Š', 'æ™‚', 'æ„Ÿã˜', 'çš„',
    'å ´åˆ', 'æ™‚é–“', 'å ´æ‰€', 'ãŠåº—', 'åº—', 'åº—èˆ—', 'åˆ©ç”¨', 'è¨ªå•', 'ãƒ¬ãƒ“ãƒ¥ãƒ¼'
])

def preprocess_review_improved(text):
    if pd.isna(text):
        return ""
    tokens = []
    for token in tokenizer.tokenize(text):
        pos = token.part_of_speech.split(',')[0]
        word = token.surface
        if pos in ['åè©', 'å½¢å®¹è©'] and word not in STOP_WORDS and len(word) > 1:
            tokens.append(word)
    return " ".join(tokens)

print("âœ… å®Œäº†")

# ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼ˆæ”¹å–„ç‰ˆï¼‰
print("\n[6/12] ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼ˆæ­£è¦åŒ–ç‰ˆï¼‰...")
mlb = MultiLabelBinarizer()
scaler = MinMaxScaler()
tfidf = TfidfVectorizer(max_features=500, ngram_range=(1, 2))  # 3000â†’500ã«å‰Šæ¸›

# ã‚«ãƒ†ã‚´ãƒª
category_features_raw = mlb.fit_transform(shop_grouped['category_list'])
# L2æ­£è¦åŒ–
category_features = normalize(category_features_raw, norm='l2')
print(f"   ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡: {category_features.shape} (L2æ­£è¦åŒ–æ¸ˆã¿)")

# æ˜Ÿè©•ä¾¡
rating_features_raw = scaler.fit_transform(shop_grouped[['star_rating']])
rating_features = normalize(rating_features_raw, norm='l2')
print(f"   æ˜Ÿè©•ä¾¡ç‰¹å¾´é‡: {rating_features.shape} (L2æ­£è¦åŒ–æ¸ˆã¿)")

# ãƒ¬ãƒ“ãƒ¥ãƒ¼
shop_grouped['processed_review'] = shop_grouped['review_text'].apply(preprocess_review_improved)
review_features_raw = tfidf.fit_transform(shop_grouped['processed_review']).toarray()
# L2æ­£è¦åŒ–
review_features = normalize(review_features_raw, norm='l2')
print(f"   ãƒ¬ãƒ“ãƒ¥ãƒ¼ç‰¹å¾´é‡: {review_features.shape} (L2æ­£è¦åŒ–æ¸ˆã¿)")
print("âœ… å®Œäº†")

# ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ç‰¹å¾´é‡ï¼ˆé‡ã¿ä»˜ãçµåˆï¼‰
print("\n[7/12] ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ç‰¹å¾´é‡è¡Œåˆ—ä½œæˆï¼ˆé‡ã¿ä»˜ãï¼‰...")

# é‡ã¿è¨­å®š
CATEGORY_WEIGHT = 2.0  # ã‚«ãƒ†ã‚´ãƒªã®é‡è¦åº¦ã‚’ä¸Šã’ã‚‹
REVIEW_WEIGHT = 1.0
RATING_WEIGHT = 1.0

# P1: ã‚«ãƒ†ã‚´ãƒª + æ˜Ÿã®æ•°
features_p1 = np.concatenate([
    category_features * CATEGORY_WEIGHT,
    rating_features * RATING_WEIGHT
], axis=1)
print(f"   P1: {features_p1.shape} (ã‚«ãƒ†ã‚´ãƒªé‡ã¿Ã—{CATEGORY_WEIGHT})")

# P2: ã‚«ãƒ†ã‚´ãƒª + ãƒ¬ãƒ“ãƒ¥ãƒ¼
features_p2 = np.concatenate([
    category_features * CATEGORY_WEIGHT,
    review_features * REVIEW_WEIGHT
], axis=1)
print(f"   P2: {features_p2.shape} (ã‚«ãƒ†ã‚´ãƒªé‡ã¿Ã—{CATEGORY_WEIGHT})")

# P3: ã‚«ãƒ†ã‚´ãƒª + ãƒ¬ãƒ“ãƒ¥ãƒ¼ + æ˜Ÿã®æ•°
features_p3 = np.concatenate([
    category_features * CATEGORY_WEIGHT,
    review_features * REVIEW_WEIGHT,
    rating_features * RATING_WEIGHT
], axis=1)
print(f"   P3: {features_p3.shape} (ã‚«ãƒ†ã‚´ãƒªé‡ã¿Ã—{CATEGORY_WEIGHT})")
print("âœ… å®Œäº†")

# æ¨è–¦é–¢æ•°
def recommend_with_filter(user_vector, feature_matrix, shop_df,
                          category_filter=None, min_rating=None, top_k=5):
    if user_vector.ndim == 1:
        user_vector = user_vector.reshape(1, -1)

    similarities = cosine_similarity(user_vector, feature_matrix)[0]
    mask = np.ones(len(shop_df), dtype=bool)

    if category_filter:
        category_mask = shop_df['category_list'].apply(
            lambda cats: category_filter in cats
        )
        mask = mask & category_mask

    if min_rating is not None:
        rating_mask = shop_df['star_rating'] >= min_rating
        mask = mask & rating_mask

    filtered_indices = np.where(mask)[0]

    if len(filtered_indices) == 0:
        return pd.DataFrame()

    filtered_similarities = similarities[filtered_indices]
    top_local_indices = np.argsort(filtered_similarities)[-top_k:][::-1]
    top_global_indices = filtered_indices[top_local_indices]

    result = shop_df.iloc[top_global_indices].copy()
    result['é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢'] = similarities[top_global_indices]

    return result[['shop_name', 'star_rating', 'category', 'é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢']]

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ™ã‚¯ãƒˆãƒ«ä½œæˆï¼ˆé‡ã¿ä»˜ãï¼‰
def create_user_vector_p1(category_query, rating_query):
    cat_list = [category_query] if isinstance(category_query, str) else category_query
    cat_vec = normalize(mlb.transform([cat_list]), norm='l2')
    rating_vec = normalize(scaler.transform([[rating_query]]), norm='l2')
    return np.concatenate([cat_vec * CATEGORY_WEIGHT, rating_vec * RATING_WEIGHT], axis=1).flatten()

def create_user_vector_p2(category_query, review_query):
    cat_list = [category_query] if isinstance(category_query, str) else category_query
    cat_vec = normalize(mlb.transform([cat_list]), norm='l2')
    processed = preprocess_review_improved(review_query)
    review_vec = normalize(tfidf.transform([processed]).toarray(), norm='l2')
    return np.concatenate([cat_vec * CATEGORY_WEIGHT, review_vec * REVIEW_WEIGHT], axis=1).flatten()

def create_user_vector_p3(category_query, review_query, rating_query):
    cat_list = [category_query] if isinstance(category_query, str) else category_query
    cat_vec = normalize(mlb.transform([cat_list]), norm='l2')
    processed = preprocess_review_improved(review_query)
    review_vec = normalize(tfidf.transform([processed]).toarray(), norm='l2')
    rating_vec = normalize(scaler.transform([[rating_query]]), norm='l2')
    return np.concatenate([
        cat_vec * CATEGORY_WEIGHT,
        review_vec * REVIEW_WEIGHT,
        rating_vec * RATING_WEIGHT
    ], axis=1).flatten()

print("\nâœ… æ¨è–¦é–¢æ•°å®šç¾©å®Œäº†")

# ãƒ†ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ª
print("\n[8/12] ãƒ†ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ªè¨­å®š...")
test_scenarios = [
    {
        'name': 'ã‚·ãƒŠãƒªã‚ª1: ã‚¤ã‚¿ãƒªã‚¢ãƒ³ã€ã‚¯ãƒªãƒ¼ãƒŸãƒ¼ãªãƒ‘ã‚¹ã‚¿',
        'category': 'ã‚¤ã‚¿ãƒªã‚¢ãƒ³',
        'review': 'ã‚¯ãƒªãƒ¼ãƒŸãƒ¼ãªãƒ‘ã‚¹ã‚¿ãŒé£Ÿã¹ãŸã„ã€‚ãƒãƒ¼ã‚ºãŸã£ã·ã‚Šã§æ¿ƒåšãªå‘³ã‚ã„',
        'rating': 3.5
    },
    {
        'name': 'ã‚·ãƒŠãƒªã‚ª2: å’Œé£Ÿã€é™ã‹ã§è½ã¡ç€ã„ãŸé›°å›²æ°—',
        'category': 'æ—¥æœ¬æ–™ç†',
        'review': 'é™ã‹ã§è½ã¡ç€ã„ãŸé›°å›²æ°—ã€‚ä¸Šå“ã§ç¹Šç´°ãªå‘³ä»˜ã‘ã€‚ä¸å¯§ãªæ¥å®¢',
        'rating': 4.0
    },
    {
        'name': 'ã‚·ãƒŠãƒªã‚ª3: æµ·é®®ã€æ–°é®®ãªåˆºèº«',
        'category': 'æµ·é®®',
        'review': 'æ–°é®®ãªåˆºèº«ãŒé£Ÿã¹ãŸã„ã€‚é­šã®ç”˜ã¿ãŒæ„Ÿã˜ã‚‰ã‚Œã‚‹ã€‚æµ·ã®å¹¸',
        'rating': 3.8
    }
]
print("âœ… å®Œäº†")

# å®Ÿé¨“å®Ÿè¡Œ
print("\n[9/12] å®Ÿé¨“å®Ÿè¡Œ...")
results = {}

for scenario in test_scenarios:
    print("\n" + "="*80)
    print(f"ğŸ” {scenario['name']}")
    print("="*80)

    cat = scenario['category']
    rev = scenario['review']
    rat = scenario['rating']

    print("\nã€P1: ã‚«ãƒ†ã‚´ãƒª + æ˜Ÿã®æ•°ã€‘")
    user_vec_p1 = create_user_vector_p1(cat, rat)
    recs_p1 = recommend_with_filter(user_vec_p1, features_p1, shop_grouped, category_filter=cat, top_k=5)
    print(recs_p1.to_string(index=False))

    print("\nã€P2: ã‚«ãƒ†ã‚´ãƒª + ãƒ¬ãƒ“ãƒ¥ãƒ¼ã€‘")
    user_vec_p2 = create_user_vector_p2(cat, rev)
    recs_p2 = recommend_with_filter(user_vec_p2, features_p2, shop_grouped, category_filter=cat, top_k=5)
    print(recs_p2.to_string(index=False))

    print("\nã€P3: ã‚«ãƒ†ã‚´ãƒª + ãƒ¬ãƒ“ãƒ¥ãƒ¼ + æ˜Ÿã®æ•°ã€‘")
    user_vec_p3 = create_user_vector_p3(cat, rev, rat)
    recs_p3 = recommend_with_filter(user_vec_p3, features_p3, shop_grouped, category_filter=cat, top_k=5)
    print(recs_p3.to_string(index=False))

    results[scenario['name']] = {'P1': recs_p1, 'P2': recs_p2, 'P3': recs_p3}

print("\nâœ… å®Œäº†")

# è©•ä¾¡
print("\n[10/12] è©•ä¾¡æŒ‡æ¨™è¨ˆç®—...")
score_stats = []
for scenario_name, patterns in results.items():
    for pattern_name, recs in patterns.items():
        if not recs.empty:
            scores = recs['é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢'].values
            score_stats.append({
                'ã‚·ãƒŠãƒªã‚ª': scenario_name,
                'ãƒ‘ã‚¿ãƒ¼ãƒ³': pattern_name,
                'å¹³å‡é¡ä¼¼åº¦': scores.mean(),
                'æœ€å¤§é¡ä¼¼åº¦': scores.max(),
                'æœ€å°é¡ä¼¼åº¦': scores.min(),
                'å¹³å‡è©•ä¾¡': recs['star_rating'].mean()
            })

stats_df = pd.DataFrame(score_stats)
print("\nğŸ“Š é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢çµ±è¨ˆ")
print(stats_df.to_string(index=False))
print("âœ… å®Œäº†")

# è²¢çŒ®åº¦åˆ†æ
print("\n[11/12] è²¢çŒ®åº¦åˆ†æ...")
contribution = []
for scenario_name in results.keys():
    p1_score = results[scenario_name]['P1']['é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢'].mean() if not results[scenario_name]['P1'].empty else 0
    p2_score = results[scenario_name]['P2']['é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢'].mean() if not results[scenario_name]['P2'].empty else 0
    p3_score = results[scenario_name]['P3']['é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢'].mean() if not results[scenario_name]['P3'].empty else 0

    contribution.append({
        'ã‚·ãƒŠãƒªã‚ª': scenario_name,
        'ãƒ¬ãƒ“ãƒ¥ãƒ¼è²¢çŒ®åº¦ (P2-P1)': p2_score - p1_score,
        'æ˜Ÿè©•ä¾¡è²¢çŒ®åº¦ (P3-P2)': p3_score - p2_score,
        'P1': p1_score,
        'P2': p2_score,
        'P3': p3_score
    })

contrib_df = pd.DataFrame(contribution)
print("\nğŸ“ˆ ç‰¹å¾´é‡è²¢çŒ®åº¦åˆ†æ")
print(contrib_df.to_string(index=False))
print("âœ… å®Œäº†")

# å¯è¦–åŒ–
print("\n[12/12] å¯è¦–åŒ–...")

fig, axes = plt.subplots(1, 3, figsize=(18, 5))
for i, scenario in enumerate(test_scenarios):
    scenario_name = scenario['name']
    if scenario_name in results:
        pattern_data = results[scenario_name]
        patterns = ['P1', 'P2', 'P3']
        avg_scores = [pattern_data[p]['é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢'].mean() if not pattern_data[p].empty else 0 for p in patterns]
        axes[i].bar(patterns, avg_scores, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])
        axes[i].set_title(f"{scenario['name'].split(':')[1].strip()}\n(Category: {scenario['category']})", fontsize=10)
        axes[i].set_ylabel('Avg Similarity', fontsize=9)
        axes[i].set_ylim(0, 1)
        axes[i].grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.savefig('/home/user/B2-store-recommend/pattern_comparison_v2.png', dpi=150, bbox_inches='tight')
print("âœ… pattern_comparison_v2.png")
plt.close()

pivot_data = stats_df.pivot_table(index='ãƒ‘ã‚¿ãƒ¼ãƒ³', columns='ã‚·ãƒŠãƒªã‚ª', values='å¹³å‡é¡ä¼¼åº¦')
fig, ax = plt.subplots(figsize=(12, 6))
pivot_data.T.plot(kind='bar', ax=ax, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])
ax.set_title('Pattern Comparison (Normalized Features)', fontsize=14, fontweight='bold')
ax.set_xlabel('Scenario', fontsize=11)
ax.set_ylabel('Avg Similarity', fontsize=11)
ax.legend(title='Pattern', fontsize=10)
ax.grid(axis='y', alpha=0.3)
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.savefig('/home/user/B2-store-recommend/scenario_comparison_v2.png', dpi=150, bbox_inches='tight')
print("âœ… scenario_comparison_v2.png")
plt.close()

# ãƒ¬ãƒãƒ¼ãƒˆ
summary_path = '/home/user/B2-store-recommend/experiment_summary_v2.txt'
with open(summary_path, 'w', encoding='utf-8') as f:
    f.write("="*80 + "\n")
    f.write("æ”¹å–„ç‰ˆv2: ç‰¹å¾´é‡æ­£è¦åŒ–ã¨ãƒãƒ©ãƒ³ã‚¹èª¿æ•´ - çµæœã‚µãƒãƒªãƒ¼\n")
    f.write("="*80 + "\n\n")

    f.write("## æ”¹å–„ç‚¹\n")
    f.write("1. ç‰¹å¾´é‡ã®L2æ­£è¦åŒ–ã‚’é©ç”¨\n")
    f.write(f"2. TF-IDFæ¬¡å…ƒæ•°ã‚’å‰Šæ¸›: 3000 â†’ 500\n")
    f.write(f"3. ã‚«ãƒ†ã‚´ãƒªã®é‡ã¿ä»˜ã‘ã‚’å¼·åŒ–: Ã—{CATEGORY_WEIGHT}\n")
    f.write("4. å„ç‰¹å¾´é‡ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ç‹¬ç«‹ã«æ­£è¦åŒ–\n\n")

    f.write("## ãƒ‡ãƒ¼ã‚¿æ¦‚è¦\n")
    f.write(f"- ãƒ¬ãƒ“ãƒ¥ãƒ¼ç·æ•°: {len(df):,}ä»¶\n")
    f.write(f"- åº—èˆ—æ•°: {len(shop_grouped):,}åº—èˆ—\n")
    f.write(f"- ã‚«ãƒ†ã‚´ãƒªæ•°: {len(mlb.classes_)}ç¨®é¡\n")
    f.write(f"- TF-IDFèªå½™æ•°: {len(tfidf.get_feature_names_out())}èª\n\n")

    f.write("## ãƒ‘ã‚¿ãƒ¼ãƒ³å®šç¾©\n")
    f.write(f"- P1: ã‚«ãƒ†ã‚´ãƒª(Ã—{CATEGORY_WEIGHT}) + æ˜Ÿ(Ã—{RATING_WEIGHT}) (æ¬¡å…ƒ: {features_p1.shape[1]})\n")
    f.write(f"- P2: ã‚«ãƒ†ã‚´ãƒª(Ã—{CATEGORY_WEIGHT}) + ãƒ¬ãƒ“ãƒ¥ãƒ¼(Ã—{REVIEW_WEIGHT}) (æ¬¡å…ƒ: {features_p2.shape[1]})\n")
    f.write(f"- P3: ã‚«ãƒ†ã‚´ãƒª(Ã—{CATEGORY_WEIGHT}) + ãƒ¬ãƒ“ãƒ¥ãƒ¼(Ã—{REVIEW_WEIGHT}) + æ˜Ÿ(Ã—{RATING_WEIGHT}) (æ¬¡å…ƒ: {features_p3.shape[1]})\n\n")

    f.write("## é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢çµ±è¨ˆ\n")
    f.write(stats_df.to_string(index=False))
    f.write("\n\n")

    f.write("## ç‰¹å¾´é‡è²¢çŒ®åº¦åˆ†æ\n")
    f.write(contrib_df.to_string(index=False))
    f.write("\n\n")

    f.write("## çµè«–\n")
    avg_review_contrib = contrib_df['ãƒ¬ãƒ“ãƒ¥ãƒ¼è²¢çŒ®åº¦ (P2-P1)'].mean()
    avg_rating_contrib = contrib_df['æ˜Ÿè©•ä¾¡è²¢çŒ®åº¦ (P3-P2)'].mean()

    f.write(f"- ãƒ¬ãƒ“ãƒ¥ãƒ¼æƒ…å ±ã®å¹³å‡è²¢çŒ®åº¦: {avg_review_contrib:+.4f}\n")
    f.write(f"- æ˜Ÿè©•ä¾¡ã®å¹³å‡è²¢çŒ®åº¦: {avg_rating_contrib:+.4f}\n")

    if avg_review_contrib > 0:
        f.write("\nâœ… ãƒ¬ãƒ“ãƒ¥ãƒ¼æƒ…å ±ã®è¿½åŠ ã«ã‚ˆã‚Šã€æ¨è–¦ç²¾åº¦ãŒå‘ä¸Š\n")
    else:
        f.write("\nâš ï¸ ãƒ¬ãƒ“ãƒ¥ãƒ¼æƒ…å ±ã«ã‚ˆã‚‹ç²¾åº¦å‘ä¸Šã¯é™å®šçš„\n")

    best_pattern = stats_df.groupby('ãƒ‘ã‚¿ãƒ¼ãƒ³')['å¹³å‡é¡ä¼¼åº¦'].mean().idxmax()
    f.write(f"\nğŸ† æœ€é«˜æ€§èƒ½: {best_pattern}\n")

    f.write("\n## å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å¹³å‡é¡ä¼¼åº¦\n")
    pattern_avg = stats_df.groupby('ãƒ‘ã‚¿ãƒ¼ãƒ³')['å¹³å‡é¡ä¼¼åº¦'].mean()
    for pattern, score in pattern_avg.items():
        f.write(f"- {pattern}: {score:.4f}\n")

print(f"âœ… {summary_path}")

print("\n" + "="*80)
print("ğŸ‰ æ”¹å–„ç‰ˆv2 å®Ÿé¨“å®Œäº†ï¼")
print("="*80)
print("\nç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«:")
print("  1. pattern_comparison_v2.png")
print("  2. scenario_comparison_v2.png")
print("  3. experiment_summary_v2.txt")
