{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f633ce42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDFモデルを学習中...\n",
      "学習完了。\n",
      "\n",
      "ユーザーの要望: 「パンケーキが美味しいお店」\n",
      "\n",
      "--- 推薦結果 TOP 10 ---\n",
      "                              shop_name  score\n",
      "オールデイダイニング グランドエール(ヴィラフォンテーヌ グランド 東京有明)    0.0\n",
      "                                ア・タ・ゴール    0.0\n",
      "                                ア・タ・ゴール    0.0\n",
      "                                ア・タ・ゴール    0.0\n",
      "                                ア・タ・ゴール    0.0\n",
      "                                ア・タ・ゴール    0.0\n",
      "                                ア・タ・ゴール    0.0\n",
      "                                ア・タ・ゴール    0.0\n",
      "                                ア・タ・ゴール    0.0\n",
      "                                ア・タ・ゴール    0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import MeCab\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# --- 準備：ユーザー入力の前処理用関数 ---\n",
    "def preprocess_query(text, tagger, stop_words):\n",
    "    \"\"\"ユーザーの入力文を、分析で使うBi-gramの文字列に変換する\"\"\"\n",
    "    words = []\n",
    "    for line in tagger.parse(text).splitlines():\n",
    "        if line == 'EOS' or line == '': continue\n",
    "        parts = line.split('\\t')\n",
    "        if len(parts) == 2:\n",
    "            if parts[1].split(',')[0] not in ['助詞', '助動詞', '接続詞', '記号']:\n",
    "                words.append(parts[0])\n",
    "\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    bigrams = [f\"{filtered_words[i]} {filtered_words[i+1]}\" for i in range(len(filtered_words) - 1)]\n",
    "    return ' '.join(bigrams)\n",
    "\n",
    "# --- メイン処理 ---\n",
    "if __name__ == '__main__':\n",
    "    # 1. 「掃除済み」のCSVファイルを読み込む\n",
    "    try:\n",
    "        df = pd.read_csv('/Users/dangararara/lecture/miraisouzou/csv/odaiba_reviews_cleaned.csv')\n",
    "    except FileNotFoundError:\n",
    "        print(\"エラー: 'reviews_cleaned.csv'が見つかりません。\")\n",
    "        exit()\n",
    "\n",
    "    df.dropna(subset=['cleaned_text'], inplace=True)\n",
    "\n",
    "    corpus = df['cleaned_text'].tolist()\n",
    "    shop_names = df['shop_name'].tolist()\n",
    "\n",
    "    if corpus:\n",
    "        # 2. お店ごとの特徴ベクトル（TF-IDF）を作成\n",
    "        print(\"TF-IDFモデルを学習中...\")\n",
    "        vectorizer = TfidfVectorizer(ngram_range=(2, 2), min_df=2, max_df=0.9)\n",
    "        tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "        print(\"学習完了。\")\n",
    "\n",
    "        # 3. MeCabとストップワードを準備\n",
    "        tagger = MeCab.Tagger()\n",
    "        stop_words = [\n",
    "            'こと', 'もの', 'それ', 'これ', 'さん', 'よう', 'ため', '的', '気', '感', 'いる', 'する', \n",
    "            'なる', 'ある', 'ない', 'くる', 'いく', '思う', '言う', '美味しい', 'うまい', '普通', \n",
    "            '良い', '悪い', '最高', '本当', '今回', '前回', '初', '再訪', '人気', '有名', '定番', \n",
    "            '満足', '好き', 'おすすめ', 'ごちそうさま', '様', '料理', '食事', 'メニュー', 'ドリンク', \n",
    "            'デザート', '一品', 'ご飯', 'ビール', 'ワイン', 'コーヒー', '肉', '魚', '野菜', \n",
    "            '食べる', '飲む', '行く', '来る', '入る', '出る', '頼む', '注文', '予約', '訪問', \n",
    "            '利用', '頂く', 'いただく', '会計', '時間', '日', '時', '分', 'お店', 'レストラン', \n",
    "            'こちら', 'ところ', '場所', '雰囲気', '店内', '外観', '席', 'カウンター', 'テーブル', \n",
    "            '個室', 'スタッフ', '店員', '自分', '私', '人', '一つ', 'たくさん', '少し', 'ちょっと', \n",
    "            'かなり', '非常', 'いろいろ', '色々', '種類', '豊富', '一番', '円', '...'\n",
    "        ]\n",
    "\n",
    "        # 推薦ロジック \n",
    "        \n",
    "        # 4. ユーザーの曖- 曖昧な要望を定義\n",
    "        user_query = \"パンケーキが美味しいお店\"\n",
    "        \n",
    "        print(f\"\\nユーザーの要望: 「{user_query}」\\n\")\n",
    "\n",
    "        # 5. ユーザーの要望を前処理し、ベクトルに変換\n",
    "        query_processed = preprocess_query(user_query, tagger, stop_words)\n",
    "        query_vector = vectorizer.transform([query_processed])\n",
    "\n",
    "        # 6. コサイン類似度を計算\n",
    "        cosine_similarities = cosine_similarity(query_vector, tfidf_matrix)\n",
    "        scores = cosine_similarities[0]\n",
    "        \n",
    "        # 7. 結果を整形してランキング表示\n",
    "        results_df = pd.DataFrame({'shop_name': shop_names, 'score': scores})\n",
    "        top_10_recommendations = results_df.sort_values(by='score', ascending=False).head(10)\n",
    "\n",
    "        print(\"--- 推薦結果 TOP 10 ---\")\n",
    "        print(top_10_recommendations.to_string(index=False))\n",
    "\n",
    "    else:\n",
    "        print(\"分析対象のデータがありません。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
