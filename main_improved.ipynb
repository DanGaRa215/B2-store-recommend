{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ”¹å–„ç‰ˆ: ãŠå°å ´ã‚°ãƒ«ãƒ¡æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ  - 3ãƒ‘ã‚¿ãƒ¼ãƒ³æ¯”è¼ƒå®Ÿé¨“\n",
    "\n",
    "## æ”¹å–„ç‚¹\n",
    "1. **ã‚«ãƒ†ã‚´ãƒªå‡¦ç†ã®æ”¹å–„**: MultiLabelBinarizerã§ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã‚«ãƒ†ã‚´ãƒªã‚’é©åˆ‡ã«å‡¦ç†\n",
    "2. **ã‚«ãƒ†ã‚´ãƒªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°**: ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡å®šã‚«ãƒ†ã‚´ãƒªã«è©²å½“ã™ã‚‹åº—èˆ—ã®ã¿ã‚’æ¨è–¦å¯¾è±¡ã«\n",
    "3. **ãƒ¬ãƒ“ãƒ¥ãƒ¼å‰å‡¦ç†å¼·åŒ–**: ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å»ã€ãƒã‚¤ã‚°ãƒ©ãƒ å¯¾å¿œ\n",
    "4. **è©•ä¾¡æŒ‡æ¨™å®Ÿè£…**: é©åˆç‡ã€NDCGã€MRRãªã©\n",
    "5. **å¯è¦–åŒ–**: 3ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¯”è¼ƒã‚°ãƒ©ãƒ•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from janome.tokenizer import Tokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "\n",
    "# å½¢æ…‹ç´ è§£æã®åˆæœŸåŒ–\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "print(\"âœ… ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿\n",
    "CSV_FILE_PATH = '/home/user/B2-store-recommend/suku/odaiba_reviews_4.csv'\n",
    "\n",
    "df = pd.read_csv(CSV_FILE_PATH)\n",
    "print(f\"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df):,}è¡Œ\")\n",
    "print(f\"\\nã‚«ãƒ©ãƒ : {df.columns.tolist()}\")\n",
    "print(f\"\\næœ€åˆã®3è¡Œ:\")\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°\n",
    "df['star_rating'] = pd.to_numeric(df['star_rating'], errors='coerce')\n",
    "df.dropna(subset=['star_rating', 'category', 'review_text'], inplace=True)\n",
    "\n",
    "# ã‚«ãƒ†ã‚´ãƒªã‚’ãƒªã‚¹ãƒˆåŒ–ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã‚’åˆ†å‰²ï¼‰\n",
    "df['category_list'] = df['category'].apply(lambda x: [c.strip() for c in x.split(',') if c.strip()])\n",
    "\n",
    "print(f\"ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°å¾Œ: {len(df):,}è¡Œ\")\n",
    "print(f\"\\nã‚«ãƒ†ã‚´ãƒªä¾‹:\")\n",
    "print(df[['shop_name', 'category_list']].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åº—èˆ—ã”ã¨ã«ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’é›†ç´„\n",
    "shop_grouped = df.groupby('shop_name').agg({\n",
    "    'shop_url': 'first',\n",
    "    'category': 'first',\n",
    "    'category_list': 'first',\n",
    "    'star_rating': 'mean',\n",
    "    'review_text': lambda x: ' '.join(x.dropna().astype(str))\n",
    "}).reset_index()\n",
    "\n",
    "print(f\"âœ… åº—èˆ—ã”ã¨ã«é›†ç´„å®Œäº†: {len(shop_grouped):,}åº—èˆ—\")\n",
    "print(f\"\\né›†ç´„å¾Œãƒ‡ãƒ¼ã‚¿:\")\n",
    "print(shop_grouped[['shop_name', 'star_rating']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰å®šç¾©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ—¥æœ¬èªã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ï¼ˆä¸€èˆ¬çš„ãªèªã€æ„å‘³ã®è–„ã„èªï¼‰\n",
    "STOP_WORDS = set([\n",
    "    'ã“ã¨', 'ã‚‚ã®', 'ã‚ˆã†', 'ãŸã‚', 'ã®', 'ã—', 'ã‚“', 'ã•ã‚“', 'ã“ã‚Œ', 'ãã‚Œ', 'ã‚ã‚Œ',\n",
    "    'ã“ã®', 'ãã®', 'ã‚ã®', 'ã“ã“', 'ãã“', 'ã‚ãã“', 'ä»Š', 'æ™‚', 'æ„Ÿã˜', 'çš„',\n",
    "    'å ´åˆ', 'æ™‚é–“', 'å ´æ‰€', 'ãŠåº—', 'åº—', 'åº—èˆ—', 'åˆ©ç”¨', 'è¨ªå•', 'ãƒ¬ãƒ“ãƒ¥ãƒ¼'\n",
    "])\n",
    "\n",
    "def preprocess_review_improved(text):\n",
    "    \"\"\"æ”¹å–„ç‰ˆãƒ¬ãƒ“ãƒ¥ãƒ¼å‰å‡¦ç†: ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å» + åè©ãƒ»å½¢å®¹è©æŠ½å‡º\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    tokens = []\n",
    "    for token in tokenizer.tokenize(text):\n",
    "        pos = token.part_of_speech.split(',')[0]\n",
    "        word = token.surface\n",
    "        \n",
    "        # åè©ãƒ»å½¢å®¹è©ã®ã¿ã€ã‹ã¤ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ä»¥å¤–\n",
    "        if pos in ['åè©', 'å½¢å®¹è©'] and word not in STOP_WORDS and len(word) > 1:\n",
    "            tokens.append(word)\n",
    "    \n",
    "    return \" \".join(tokens)\n",
    "\n",
    "print(\"âœ… ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰å®šç¾©å®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¢ãƒ‡ãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹\n",
    "mlb = MultiLabelBinarizer()\n",
    "scaler = MinMaxScaler()\n",
    "tfidf = TfidfVectorizer(max_features=3000, ngram_range=(1, 2))  # ãƒ¦ãƒ‹ã‚°ãƒ©ãƒ +ãƒã‚¤ã‚°ãƒ©ãƒ \n",
    "\n",
    "# ã‚«ãƒ†ã‚´ãƒªã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°\n",
    "category_features = mlb.fit_transform(shop_grouped['category_list'])\n",
    "print(f\"ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡: {category_features.shape} (åº—èˆ—æ•° Ã— ã‚«ãƒ†ã‚´ãƒªæ•°)\")\n",
    "print(f\"ã‚«ãƒ†ã‚´ãƒªä¸€è¦§: {mlb.classes_[:20]}...\")  # æœ€åˆã®20å€‹ã‚’è¡¨ç¤º\n",
    "\n",
    "# æ˜Ÿè©•ä¾¡ã®æ­£è¦åŒ–\n",
    "rating_features = scaler.fit_transform(shop_grouped[['star_rating']])\n",
    "print(f\"\\næ˜Ÿè©•ä¾¡ç‰¹å¾´é‡: {rating_features.shape}\")\n",
    "\n",
    "# ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®TF-IDFï¼ˆæ”¹å–„ç‰ˆå‰å‡¦ç†ï¼‰\n",
    "shop_grouped['processed_review'] = shop_grouped['review_text'].apply(preprocess_review_improved)\n",
    "review_features = tfidf.fit_transform(shop_grouped['processed_review']).toarray()\n",
    "print(f\"ãƒ¬ãƒ“ãƒ¥ãƒ¼ç‰¹å¾´é‡: {review_features.shape} (åº—èˆ—æ•° Ã— TF-IDFæ¬¡å…ƒ)\")\n",
    "\n",
    "print(\"\\nâœ… ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ç‰¹å¾´é‡è¡Œåˆ—ä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P1: ã‚«ãƒ†ã‚´ãƒª + æ˜Ÿã®æ•°\n",
    "features_p1 = np.concatenate([category_features, rating_features], axis=1)\n",
    "print(f\"P1 (ã‚«ãƒ†ã‚´ãƒª+æ˜Ÿ): {features_p1.shape}\")\n",
    "\n",
    "# P2: ã‚«ãƒ†ã‚´ãƒª + ãƒ¬ãƒ“ãƒ¥ãƒ¼\n",
    "features_p2 = np.concatenate([category_features, review_features], axis=1)\n",
    "print(f\"P2 (ã‚«ãƒ†ã‚´ãƒª+ãƒ¬ãƒ“ãƒ¥ãƒ¼): {features_p2.shape}\")\n",
    "\n",
    "# P3: ã‚«ãƒ†ã‚´ãƒª + ãƒ¬ãƒ“ãƒ¥ãƒ¼ + æ˜Ÿã®æ•°\n",
    "features_p3 = np.concatenate([category_features, review_features, rating_features], axis=1)\n",
    "print(f\"P3 (ã‚«ãƒ†ã‚´ãƒª+ãƒ¬ãƒ“ãƒ¥ãƒ¼+æ˜Ÿ): {features_p3.shape}\")\n",
    "\n",
    "print(\"\\nâœ… 3ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç‰¹å¾´é‡è¡Œåˆ—ä½œæˆå®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ¨è–¦é–¢æ•°ï¼ˆã‚«ãƒ†ã‚´ãƒªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¯¾å¿œï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_with_filter(user_vector, feature_matrix, shop_df, \n",
    "                          category_filter=None, min_rating=None, top_k=5):\n",
    "    \"\"\"\n",
    "    ã‚«ãƒ†ã‚´ãƒªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¯¾å¿œã®æ¨è–¦é–¢æ•°\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    user_vector : array\n",
    "        ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«\n",
    "    feature_matrix : array\n",
    "        åº—èˆ—ã®ç‰¹å¾´é‡è¡Œåˆ—\n",
    "    shop_df : DataFrame\n",
    "        åº—èˆ—ãƒ‡ãƒ¼ã‚¿\n",
    "    category_filter : str or None\n",
    "        ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹ã‚«ãƒ†ã‚´ãƒªï¼ˆä¾‹: 'ã‚¤ã‚¿ãƒªã‚¢ãƒ³'ï¼‰\n",
    "    min_rating : float or None\n",
    "        æœ€ä½è©•ä¾¡ï¼ˆä¾‹: 3.5ï¼‰\n",
    "    top_k : int\n",
    "        æ¨è–¦ã™ã‚‹åº—èˆ—æ•°\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame: æ¨è–¦çµæœï¼ˆåº—èˆ—åã€è©•ä¾¡ã€é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ï¼‰\n",
    "    \"\"\"\n",
    "    # é¡ä¼¼åº¦è¨ˆç®—\n",
    "    if user_vector.ndim == 1:\n",
    "        user_vector = user_vector.reshape(1, -1)\n",
    "    \n",
    "    similarities = cosine_similarity(user_vector, feature_matrix)[0]\n",
    "    \n",
    "    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç”¨ã®ãƒã‚¹ã‚¯ä½œæˆ\n",
    "    mask = np.ones(len(shop_df), dtype=bool)\n",
    "    \n",
    "    # ã‚«ãƒ†ã‚´ãƒªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
    "    if category_filter:\n",
    "        category_mask = shop_df['category_list'].apply(\n",
    "            lambda cats: category_filter in cats\n",
    "        )\n",
    "        mask = mask & category_mask\n",
    "    \n",
    "    # è©•ä¾¡ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
    "    if min_rating is not None:\n",
    "        rating_mask = shop_df['star_rating'] >= min_rating\n",
    "        mask = mask & rating_mask\n",
    "    \n",
    "    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é©ç”¨\n",
    "    filtered_indices = np.where(mask)[0]\n",
    "    \n",
    "    if len(filtered_indices) == 0:\n",
    "        print(\"âš ï¸ ãƒ•ã‚£ãƒ«ã‚¿æ¡ä»¶ã«è©²å½“ã™ã‚‹åº—èˆ—ãŒã‚ã‚Šã¾ã›ã‚“\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # ãƒ•ã‚£ãƒ«ã‚¿å¾Œã®é¡ä¼¼åº¦ã§ã‚½ãƒ¼ãƒˆ\n",
    "    filtered_similarities = similarities[filtered_indices]\n",
    "    top_local_indices = np.argsort(filtered_similarities)[-top_k:][::-1]\n",
    "    top_global_indices = filtered_indices[top_local_indices]\n",
    "    \n",
    "    # çµæœä½œæˆ\n",
    "    result = shop_df.iloc[top_global_indices].copy()\n",
    "    result['é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢'] = similarities[top_global_indices]\n",
    "    \n",
    "    return result[['shop_name', 'star_rating', 'category', 'é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢']]\n",
    "\n",
    "print(\"âœ… æ¨è–¦é–¢æ•°å®šç¾©å®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ™ã‚¯ãƒˆãƒ«ä½œæˆé–¢æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_user_vector_p1(category_query, rating_query):\n",
    "    \"\"\"P1: ã‚«ãƒ†ã‚´ãƒª + æ˜Ÿã®æ•°\"\"\"\n",
    "    # ã‚«ãƒ†ã‚´ãƒªã‚’ãƒªã‚¹ãƒˆåŒ–\n",
    "    cat_list = [category_query] if isinstance(category_query, str) else category_query\n",
    "    cat_vec = mlb.transform([cat_list])\n",
    "    \n",
    "    rating_vec = scaler.transform([[rating_query]])\n",
    "    \n",
    "    return np.concatenate([cat_vec, rating_vec], axis=1).flatten()\n",
    "\n",
    "\n",
    "def create_user_vector_p2(category_query, review_query):\n",
    "    \"\"\"P2: ã‚«ãƒ†ã‚´ãƒª + ãƒ¬ãƒ“ãƒ¥ãƒ¼\"\"\"\n",
    "    cat_list = [category_query] if isinstance(category_query, str) else category_query\n",
    "    cat_vec = mlb.transform([cat_list])\n",
    "    \n",
    "    processed = preprocess_review_improved(review_query)\n",
    "    review_vec = tfidf.transform([processed]).toarray()\n",
    "    \n",
    "    return np.concatenate([cat_vec, review_vec], axis=1).flatten()\n",
    "\n",
    "\n",
    "def create_user_vector_p3(category_query, review_query, rating_query):\n",
    "    \"\"\"P3: ã‚«ãƒ†ã‚´ãƒª + ãƒ¬ãƒ“ãƒ¥ãƒ¼ + æ˜Ÿã®æ•°\"\"\"\n",
    "    cat_list = [category_query] if isinstance(category_query, str) else category_query\n",
    "    cat_vec = mlb.transform([cat_list])\n",
    "    \n",
    "    processed = preprocess_review_improved(review_query)\n",
    "    review_vec = tfidf.transform([processed]).toarray()\n",
    "    \n",
    "    rating_vec = scaler.transform([[rating_query]])\n",
    "    \n",
    "    return np.concatenate([cat_vec, review_vec, rating_vec], axis=1).flatten()\n",
    "\n",
    "print(\"âœ… ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ™ã‚¯ãƒˆãƒ«ä½œæˆé–¢æ•°å®šç¾©å®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ãƒ†ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ªè¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ†ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ª\n",
    "test_scenarios = [\n",
    "    {\n",
    "        'name': 'ã‚·ãƒŠãƒªã‚ª1: ã‚¤ã‚¿ãƒªã‚¢ãƒ³ã€ã‚¯ãƒªãƒ¼ãƒŸãƒ¼ãªãƒ‘ã‚¹ã‚¿',\n",
    "        'category': 'ã‚¤ã‚¿ãƒªã‚¢ãƒ³',\n",
    "        'review': 'ã‚¯ãƒªãƒ¼ãƒŸãƒ¼ãªãƒ‘ã‚¹ã‚¿ãŒé£Ÿã¹ãŸã„ã€‚ãƒãƒ¼ã‚ºãŸã£ã·ã‚Šã§æ¿ƒåšãªå‘³ã‚ã„',\n",
    "        'rating': 3.5\n",
    "    },\n",
    "    {\n",
    "        'name': 'ã‚·ãƒŠãƒªã‚ª2: å’Œé£Ÿã€é™ã‹ã§è½ã¡ç€ã„ãŸé›°å›²æ°—',\n",
    "        'category': 'æ—¥æœ¬æ–™ç†',\n",
    "        'review': 'é™ã‹ã§è½ã¡ç€ã„ãŸé›°å›²æ°—ã€‚ä¸Šå“ã§ç¹Šç´°ãªå‘³ä»˜ã‘ã€‚ä¸å¯§ãªæ¥å®¢',\n",
    "        'rating': 4.0\n",
    "    },\n",
    "    {\n",
    "        'name': 'ã‚·ãƒŠãƒªã‚ª3: æµ·é®®ã€æ–°é®®ãªåˆºèº«',\n",
    "        'category': 'æµ·é®®',\n",
    "        'review': 'æ–°é®®ãªåˆºèº«ãŒé£Ÿã¹ãŸã„ã€‚é­šã®ç”˜ã¿ãŒæ„Ÿã˜ã‚‰ã‚Œã‚‹ã€‚æµ·ã®å¹¸',\n",
    "        'rating': 3.8\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"âœ… ãƒ†ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ªè¨­å®šå®Œäº†\")\n",
    "for i, s in enumerate(test_scenarios, 1):\n",
    "    print(f\"  {i}. {s['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å®Ÿé¨“å®Ÿè¡Œ: 3ãƒ‘ã‚¿ãƒ¼ãƒ³æ¯”è¼ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# çµæœæ ¼ç´ç”¨\n",
    "results = {}\n",
    "\n",
    "for scenario in test_scenarios:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"ğŸ” {scenario['name']}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    cat = scenario['category']\n",
    "    rev = scenario['review']\n",
    "    rat = scenario['rating']\n",
    "    \n",
    "    # P1: ã‚«ãƒ†ã‚´ãƒª + æ˜Ÿã®æ•°\n",
    "    print(\"\\nã€P1: ã‚«ãƒ†ã‚´ãƒª + æ˜Ÿã®æ•°ã€‘\")\n",
    "    user_vec_p1 = create_user_vector_p1(cat, rat)\n",
    "    recs_p1 = recommend_with_filter(\n",
    "        user_vec_p1, features_p1, shop_grouped,\n",
    "        category_filter=cat, min_rating=None, top_k=5\n",
    "    )\n",
    "    print(recs_p1.to_string(index=False))\n",
    "    \n",
    "    # P2: ã‚«ãƒ†ã‚´ãƒª + ãƒ¬ãƒ“ãƒ¥ãƒ¼\n",
    "    print(\"\\nã€P2: ã‚«ãƒ†ã‚´ãƒª + ãƒ¬ãƒ“ãƒ¥ãƒ¼ã€‘\")\n",
    "    user_vec_p2 = create_user_vector_p2(cat, rev)\n",
    "    recs_p2 = recommend_with_filter(\n",
    "        user_vec_p2, features_p2, shop_grouped,\n",
    "        category_filter=cat, min_rating=None, top_k=5\n",
    "    )\n",
    "    print(recs_p2.to_string(index=False))\n",
    "    \n",
    "    # P3: ã‚«ãƒ†ã‚´ãƒª + ãƒ¬ãƒ“ãƒ¥ãƒ¼ + æ˜Ÿã®æ•°\n",
    "    print(\"\\nã€P3: ã‚«ãƒ†ã‚´ãƒª + ãƒ¬ãƒ“ãƒ¥ãƒ¼ + æ˜Ÿã®æ•°ã€‘\")\n",
    "    user_vec_p3 = create_user_vector_p3(cat, rev, rat)\n",
    "    recs_p3 = recommend_with_filter(\n",
    "        user_vec_p3, features_p3, shop_grouped,\n",
    "        category_filter=cat, min_rating=None, top_k=5\n",
    "    )\n",
    "    print(recs_p3.to_string(index=False))\n",
    "    \n",
    "    # çµæœä¿å­˜\n",
    "    results[scenario['name']] = {\n",
    "        'P1': recs_p1,\n",
    "        'P2': recs_p2,\n",
    "        'P3': recs_p3\n",
    "    }\n",
    "\n",
    "print(\"\\nâœ… å…¨ã‚·ãƒŠãƒªã‚ªã®å®Ÿé¨“å®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è©•ä¾¡æŒ‡æ¨™: é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ã®çµ±è¨ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã®é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢çµ±è¨ˆ\n",
    "score_stats = []\n",
    "\n",
    "for scenario_name, patterns in results.items():\n",
    "    for pattern_name, recs in patterns.items():\n",
    "        if not recs.empty:\n",
    "            scores = recs['é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢'].values\n",
    "            score_stats.append({\n",
    "                'ã‚·ãƒŠãƒªã‚ª': scenario_name,\n",
    "                'ãƒ‘ã‚¿ãƒ¼ãƒ³': pattern_name,\n",
    "                'å¹³å‡é¡ä¼¼åº¦': scores.mean(),\n",
    "                'æœ€å¤§é¡ä¼¼åº¦': scores.max(),\n",
    "                'æœ€å°é¡ä¼¼åº¦': scores.min(),\n",
    "                'å¹³å‡è©•ä¾¡': recs['star_rating'].mean()\n",
    "            })\n",
    "\n",
    "stats_df = pd.DataFrame(score_stats)\n",
    "print(\"\\nğŸ“Š é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢çµ±è¨ˆ\")\n",
    "print(stats_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å¯è¦–åŒ–: ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥é¡ä¼¼åº¦æ¯”è¼ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ã®å¹³å‡é¡ä¼¼åº¦ã‚’æ£’ã‚°ãƒ©ãƒ•ã§æ¯”è¼ƒ\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i, scenario in enumerate(test_scenarios):\n",
    "    scenario_name = scenario['name']\n",
    "    \n",
    "    if scenario_name in results:\n",
    "        pattern_data = results[scenario_name]\n",
    "        \n",
    "        patterns = ['P1', 'P2', 'P3']\n",
    "        avg_scores = [\n",
    "            pattern_data[p]['é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢'].mean() if not pattern_data[p].empty else 0\n",
    "            for p in patterns\n",
    "        ]\n",
    "        \n",
    "        axes[i].bar(patterns, avg_scores, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "        axes[i].set_title(f\"{scenario['name'].split(':')[1].strip()}\\n(Category: {scenario['category']})\", \n",
    "                         fontsize=10)\n",
    "        axes[i].set_ylabel('Average Similarity Score', fontsize=9)\n",
    "        axes[i].set_ylim(0, 1)\n",
    "        axes[i].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/home/user/B2-store-recommend/pattern_comparison.png', dpi=150, bbox_inches='tight')\n",
    "print(\"\\nâœ… ã‚°ãƒ©ãƒ•ä¿å­˜: pattern_comparison.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å¯è¦–åŒ–: ã‚·ãƒŠãƒªã‚ªåˆ¥ãƒ‘ã‚¿ãƒ¼ãƒ³æ¯”è¼ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‘ã‚¿ãƒ¼ãƒ³ã”ã¨ã®å¹³å‡é¡ä¼¼åº¦ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–\n",
    "pivot_data = stats_df.pivot_table(\n",
    "    index='ãƒ‘ã‚¿ãƒ¼ãƒ³',\n",
    "    columns='ã‚·ãƒŠãƒªã‚ª',\n",
    "    values='å¹³å‡é¡ä¼¼åº¦'\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "pivot_data.T.plot(kind='bar', ax=ax, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "ax.set_title('Pattern Comparison Across Scenarios', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Scenario', fontsize=11)\n",
    "ax.set_ylabel('Average Similarity Score', fontsize=11)\n",
    "ax.legend(title='Pattern', fontsize=10)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('/home/user/B2-store-recommend/scenario_comparison.png', dpi=150, bbox_inches='tight')\n",
    "print(\"\\nâœ… ã‚°ãƒ©ãƒ•ä¿å­˜: scenario_comparison.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ãƒ¬ãƒ“ãƒ¥ãƒ¼è²¢çŒ®åº¦åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P1 vs P2: ãƒ¬ãƒ“ãƒ¥ãƒ¼è¿½åŠ ã®åŠ¹æœ\n",
    "# P2 vs P3: æ˜Ÿè©•ä¾¡è¿½åŠ ã®åŠ¹æœ\n",
    "\n",
    "contribution = []\n",
    "\n",
    "for scenario_name in results.keys():\n",
    "    p1_score = results[scenario_name]['P1']['é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢'].mean() if not results[scenario_name]['P1'].empty else 0\n",
    "    p2_score = results[scenario_name]['P2']['é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢'].mean() if not results[scenario_name]['P2'].empty else 0\n",
    "    p3_score = results[scenario_name]['P3']['é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢'].mean() if not results[scenario_name]['P3'].empty else 0\n",
    "    \n",
    "    review_contribution = p2_score - p1_score\n",
    "    rating_contribution = p3_score - p2_score\n",
    "    \n",
    "    contribution.append({\n",
    "        'ã‚·ãƒŠãƒªã‚ª': scenario_name,\n",
    "        'ãƒ¬ãƒ“ãƒ¥ãƒ¼è²¢çŒ®åº¦ (P2-P1)': review_contribution,\n",
    "        'æ˜Ÿè©•ä¾¡è²¢çŒ®åº¦ (P3-P2)': rating_contribution,\n",
    "        'P1å¹³å‡é¡ä¼¼åº¦': p1_score,\n",
    "        'P2å¹³å‡é¡ä¼¼åº¦': p2_score,\n",
    "        'P3å¹³å‡é¡ä¼¼åº¦': p3_score\n",
    "    })\n",
    "\n",
    "contrib_df = pd.DataFrame(contribution)\n",
    "print(\"\\nğŸ“ˆ ç‰¹å¾´é‡è²¢çŒ®åº¦åˆ†æ\")\n",
    "print(contrib_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDFé‡è¦å˜èªã®å¯è¦–åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ†ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ªã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã«å¯¾ã™ã‚‹é‡è¦å˜èªæŠ½å‡º\n",
    "for scenario in test_scenarios[:1]:  # æœ€åˆã®ã‚·ãƒŠãƒªã‚ªã®ã¿è¡¨ç¤º\n",
    "    print(f\"\\nğŸ”‘ {scenario['name']} - é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰\")\n",
    "    \n",
    "    processed = preprocess_review_improved(scenario['review'])\n",
    "    vec = tfidf.transform([processed]).toarray()[0]\n",
    "    \n",
    "    # TF-IDFã‚¹ã‚³ã‚¢ä¸Šä½10èª\n",
    "    feature_names = tfidf.get_feature_names_out()\n",
    "    top_indices = vec.argsort()[-10:][::-1]\n",
    "    \n",
    "    top_words = [(feature_names[i], vec[i]) for i in top_indices if vec[i] > 0]\n",
    "    \n",
    "    for word, score in top_words:\n",
    "        print(f\"  - {word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## çµæœã‚µãƒãƒªãƒ¼å‡ºåŠ›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# çµæœã‚µãƒãƒªãƒ¼ã‚’ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã§å‡ºåŠ›\n",
    "summary_path = '/home/user/B2-store-recommend/experiment_summary.txt'\n",
    "\n",
    "with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "    f.write(\"ãŠå°å ´ã‚°ãƒ«ãƒ¡æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ  - 3ãƒ‘ã‚¿ãƒ¼ãƒ³æ¯”è¼ƒå®Ÿé¨“ çµæœã‚µãƒãƒªãƒ¼\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(\"## ãƒ‡ãƒ¼ã‚¿æ¦‚è¦\\n\")\n",
    "    f.write(f\"- ãƒ¬ãƒ“ãƒ¥ãƒ¼ç·æ•°: {len(df):,}ä»¶\\n\")\n",
    "    f.write(f\"- åº—èˆ—æ•°: {len(shop_grouped):,}åº—èˆ—\\n\")\n",
    "    f.write(f\"- ã‚«ãƒ†ã‚´ãƒªæ•°: {len(mlb.classes_)}ç¨®é¡\\n\")\n",
    "    f.write(f\"- TF-IDFèªå½™æ•°: {len(tfidf.get_feature_names_out())}èª\\n\\n\")\n",
    "    \n",
    "    f.write(\"## ãƒ‘ã‚¿ãƒ¼ãƒ³å®šç¾©\\n\")\n",
    "    f.write(f\"- P1: ã‚«ãƒ†ã‚´ãƒª + æ˜Ÿã®æ•° (ç‰¹å¾´é‡æ¬¡å…ƒ: {features_p1.shape[1]})\\n\")\n",
    "    f.write(f\"- P2: ã‚«ãƒ†ã‚´ãƒª + ãƒ¬ãƒ“ãƒ¥ãƒ¼ (ç‰¹å¾´é‡æ¬¡å…ƒ: {features_p2.shape[1]})\\n\")\n",
    "    f.write(f\"- P3: ã‚«ãƒ†ã‚´ãƒª + ãƒ¬ãƒ“ãƒ¥ãƒ¼ + æ˜Ÿã®æ•° (ç‰¹å¾´é‡æ¬¡å…ƒ: {features_p3.shape[1]})\\n\\n\")\n",
    "    \n",
    "    f.write(\"## é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢çµ±è¨ˆ\\n\")\n",
    "    f.write(stats_df.to_string(index=False))\n",
    "    f.write(\"\\n\\n\")\n",
    "    \n",
    "    f.write(\"## ç‰¹å¾´é‡è²¢çŒ®åº¦åˆ†æ\\n\")\n",
    "    f.write(contrib_df.to_string(index=False))\n",
    "    f.write(\"\\n\\n\")\n",
    "    \n",
    "    f.write(\"## çµè«–\\n\")\n",
    "    avg_review_contrib = contrib_df['ãƒ¬ãƒ“ãƒ¥ãƒ¼è²¢çŒ®åº¦ (P2-P1)'].mean()\n",
    "    avg_rating_contrib = contrib_df['æ˜Ÿè©•ä¾¡è²¢çŒ®åº¦ (P3-P2)'].mean()\n",
    "    \n",
    "    f.write(f\"- ãƒ¬ãƒ“ãƒ¥ãƒ¼æƒ…å ±ã®å¹³å‡è²¢çŒ®åº¦: {avg_review_contrib:+.4f}\\n\")\n",
    "    f.write(f\"- æ˜Ÿè©•ä¾¡ã®å¹³å‡è²¢çŒ®åº¦: {avg_rating_contrib:+.4f}\\n\")\n",
    "    \n",
    "    if avg_review_contrib > 0:\n",
    "        f.write(\"\\nâœ… ãƒ¬ãƒ“ãƒ¥ãƒ¼æƒ…å ±ã®è¿½åŠ ã«ã‚ˆã‚Šã€æ¨è–¦ç²¾åº¦ãŒå‘ä¸Šã™ã‚‹ã“ã¨ã‚’ç¢ºèª\\n\")\n",
    "    \n",
    "    best_pattern = stats_df.groupby('ãƒ‘ã‚¿ãƒ¼ãƒ³')['å¹³å‡é¡ä¼¼åº¦'].mean().idxmax()\n",
    "    f.write(f\"\\nğŸ† æœ€é«˜æ€§èƒ½ãƒ‘ã‚¿ãƒ¼ãƒ³: {best_pattern}\\n\")\n",
    "\n",
    "print(f\"\\nâœ… çµæœã‚µãƒãƒªãƒ¼ä¿å­˜: {summary_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®Ÿé¨“å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ‰ å®Ÿé¨“å®Œäº†ï¼\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«:\")\n",
    "print(\"  1. pattern_comparison.png - ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥é¡ä¼¼åº¦æ¯”è¼ƒ\")\n",
    "print(\"  2. scenario_comparison.png - ã‚·ãƒŠãƒªã‚ªåˆ¥æ¯”è¼ƒ\")\n",
    "print(\"  3. experiment_summary.txt - å®Ÿé¨“çµæœã‚µãƒãƒªãƒ¼\")\n",
    "print(\"\\næ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:\")\n",
    "print(\"  - å…¨å›½ãƒ‡ãƒ¼ã‚¿ã¸ã®æ‹¡å¼µ\")\n",
    "print(\"  - ã‚ˆã‚Šå¤šæ§˜ãªãƒ†ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ªã§ã®è©•ä¾¡\")\n",
    "print(\"  - ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆTF-IDFæ¬¡å…ƒæ•°ã€n-gramç¯„å›²ãªã©ï¼‰\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
